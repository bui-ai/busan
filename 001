import streamlit as st
from dotenv import load_dotenv
from langchain.schema import HumanMessage, AIMessage, SystemMessage
from langchain_ollama import ChatOllama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
import os
import tempfile
import pdfplumber
import pandas as pd
from fuzzywuzzy import process
import pickle
import glob
import json

# Load environment variables from a .env file
load_dotenv()

# ğŸ’¾ ì•± ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì • (ë¡œì»¬)
DATA_DIR = os.path.join(os.path.expanduser("~"), "chatbot_data")
TABLES_DIR = os.path.join(DATA_DIR, "tables")
CHROMA_DIR = os.path.join(DATA_DIR, "chroma_db")

# ğŸ’¾ ì•± ì‹œì‘ ì‹œ í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
def create_data_directories():
    for directory in [DATA_DIR, TABLES_DIR, CHROMA_DIR]:
        if not os.path.exists(directory):
            os.makedirs(directory)

def configure_page():
    st.set_page_config(
        page_title="ë„ì‹œí˜ì‹ ê· í˜•ì‹¤ Chat-Bot",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.title("ğŸ¤– ë„ì‹œí˜ì‹ ê· í˜•ì‹¤ Chat-Bot ğŸ“")
    with st.expander("Check State"):
        st.write(st.session_state)

def clean_column_names(df):
    """ì»¬ëŸ¼ëª…ì´ ì¤‘ë³µë˜ê±°ë‚˜ Noneì¼ ê²½ìš° ìë™ìœ¼ë¡œ ìˆ˜ì •"""
    new_columns = []
    column_counts = {}

    for col in df.columns:
        if col is None or col == "":  # ë¹ˆ ì»¬ëŸ¼ëª… ì²˜ë¦¬
            col = "Unnamed"

        if col in column_counts:
            column_counts[col] += 1
            new_col = f"{col}_{column_counts[col]}"  # ì¤‘ë³µ ë°©ì§€ (ì˜ˆ: "ìœ„ì›íšŒëª…_1")
        else:
            column_counts[col] = 0
            new_col = col

        new_columns.append(new_col)

    df.columns = new_columns
    return df

def extract_tables_from_pdf(uploaded_file):
    """PDFì—ì„œ í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ ê°œë³„ì ì¸ pandas DataFrameìœ¼ë¡œ ì €ì¥"""
    try:
        tables = []
        with pdfplumber.open(uploaded_file) as pdf:
            for page in pdf.pages:
                extracted_tables = page.extract_tables()
                for table in extracted_tables:
                    if table:  # ë¹ˆ í…Œì´ë¸” ì œì™¸
                        df = pd.DataFrame(table)

                        # ğŸ”¹ ì²« ë²ˆì§¸ í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì„¤ì •
                        df.columns = df.iloc[0]  
                        df = df[1:].reset_index(drop=True)  # ì²« ë²ˆì§¸ í–‰ ì‚­ì œ í›„ ì¬ì •ë ¬

                        # ğŸ”¹ ì²« ë²ˆì§¸ ì—´ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì • (í–‰ ì´ë¦„ ìë™ ì •ë¦¬)
                        df = df.set_index(df.columns[0])

                        df = clean_column_names(df)  # ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
                        tables.append(df)

        return tables if tables else None
    except Exception as e:
        st.error(f"âŒ í‘œ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def search_in_tables(tables, query):
    """ê° í‘œì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ê²€ìƒ‰"""
    matching_results = []

    if tables:
        for idx, df in enumerate(tables):
            # ğŸ”¹ ê°€ì¥ ìœ ì‚¬í•œ ì»¬ëŸ¼ ì°¾ê¸° (ì˜ˆ: "í•˜ë£¨ ì´ìš©ëŸ‰" â†’ "1ì¼ í†µí–‰ëŸ‰ (ëŒ€)")
            best_match_col, score_col = process.extractOne(query, df.columns) if not df.columns.empty else (None, 0)

            # ğŸ”¹ ê°€ì¥ ìœ ì‚¬í•œ í–‰ ì°¾ê¸° (ì˜ˆ: "ê´‘ì•ˆëŒ€êµ" â†’ "ê´‘ì•ˆëŒ€ë¡œ")
            best_match_row, score_row = process.extractOne(query, df.index) if not df.index.empty else (None, 0)

            # ğŸ”¹ ê²€ìƒ‰ ì¡°ê±´ ë§Œì¡±í•˜ë©´ ê²°ê³¼ ë°˜í™˜
            if score_col > 70 and score_row > 70:
                result_value = df.loc[best_match_row, best_match_col]
                result_df = pd.DataFrame({"í•­ëª©": [f"{best_match_row}ì˜ {best_match_col}"], "ê°’": [result_value]})
                matching_results.append((idx, result_df))

    return matching_results  # ê²€ìƒ‰ëœ í‘œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def search_in_dataframe(df, query):
    """ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ pandas DataFrameì—ì„œ ê²€ìƒ‰ """
    if df is not None and not df.empty:
        results = df[df.apply(lambda row: row.astype(str).str.contains(query, case=False, na=False).any(), axis=1)]
        return results
    return pd.DataFrame()  # ê´€ë ¨ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜

# ğŸ’¾ í‘œ ë°ì´í„° ì €ì¥ í•¨ìˆ˜
def save_tables(tables, filename):
    """í‘œ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not tables:
        return False
    
    tables_info = []
    for i, df in enumerate(tables):
        table_path = os.path.join(TABLES_DIR, f"{filename}_table_{i}.csv")
        df.to_csv(table_path)
        tables_info.append(table_path)
    
    # í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ì €ì¥ (íŒŒì¼ëª…ê³¼ í…Œì´ë¸” ì •ë³´)
    metadata_path = os.path.join(TABLES_DIR, f"{filename}_metadata.json")
    with open(metadata_path, 'w') as f:
        json.dump({
            "filename": filename,
            "tables": tables_info,
            "count": len(tables)
        }, f)
    
    return True

# ğŸ’¾ ì €ì¥ëœ í‘œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_saved_tables():
    """ì €ì¥ëœ ëª¨ë“  í‘œ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    tables = []
    metadata_files = glob.glob(os.path.join(TABLES_DIR, "*_metadata.json"))
    
    if not metadata_files:
        return None
    
    for metadata_file in metadata_files:
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
            
        for table_path in metadata["tables"]:
            if os.path.exists(table_path):
                try:
                    df = pd.read_csv(table_path, index_col=0)
                    tables.append(df)
                except Exception as e:
                    st.warning(f"âš ï¸ í…Œì´ë¸” ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return tables if tables else None

# ğŸ’¾ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ í•¨ìˆ˜
def load_vector_store(embeddings):
    """ì €ì¥ëœ Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œ"""
    if os.path.exists(CHROMA_DIR) and os.listdir(CHROMA_DIR):  # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°
        try:
            vector_store = Chroma(
                persist_directory=CHROMA_DIR,
                embedding_function=embeddings
            )
            return vector_store
        except Exception as e:
            st.error(f"âŒ ë²¡í„° DB ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    return None

def handle_sidebar():
    st.sidebar.header("ğŸ”§ ì„¤ì •")

    selected_model = st.sidebar.selectbox(
        "ğŸ’¡ ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ", ("EXAONE-3.5-2.4B-Instruct", "EXAONE-3.5-7.8B-Instruct")
    )
    st.session_state.model = selected_model

    st.sidebar.divider()

    k_value = st.sidebar.slider("ğŸ” ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (k)", min_value=1, max_value=10, value=5)
    lambda_mult_value = st.sidebar.slider("ğŸšï¸ MMR Lambda (0: ë‹¤ì–‘ì„± â†‘, 1: ìœ ì‚¬ì„± â†‘)", 
                                          min_value=0.0, max_value=1.0, value=0.5, step=0.1)

    st.sidebar.divider()

    # ê¸°ì¡´ ë°ì´í„° ìƒíƒœ í‘œì‹œ
    has_existing_data = False
    if os.path.exists(CHROMA_DIR) and os.listdir(CHROMA_DIR):
        st.sidebar.success("âœ… ê¸°ì¡´ ë²¡í„° DBê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        has_existing_data = True
    
    saved_tables = glob.glob(os.path.join(TABLES_DIR, "*_metadata.json"))
    if saved_tables:
        st.sidebar.success(f"âœ… {len(saved_tables)}ê°œì˜ ê¸°ì¡´ í‘œ ë°ì´í„°ì…‹ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        has_existing_data = True

    if not has_existing_data:
        st.sidebar.warning("âš ï¸ ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")

    # PDF ì—…ë¡œë“œ ì„¹ì…˜
    uploaded_file = st.sidebar.file_uploader("ğŸ“‚ PDF ì—…ë¡œë“œ", type=["pdf"])
    if uploaded_file:
        file_name = os.path.splitext(uploaded_file.name)[0]  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
        
        with st.spinner("ğŸ“– PDF ì²˜ë¦¬ ì¤‘..."):
            try:
                documents = load_pdf(uploaded_file)
                texts = split_text(documents)
                embeddings = get_embeddings()
                
                # ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥
                vector_store = Chroma.from_documents(
                    documents=texts,
                    embedding=embeddings,
                    persist_directory=CHROMA_DIR
                )
                vector_store.persist()  # ë””ìŠ¤í¬ì— ì €ì¥
                st.session_state.vector_store = vector_store

                # í‘œ ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
                tables = extract_tables_from_pdf(uploaded_file)
                if tables:
                    st.session_state.tables = tables
                    # í‘œ ë°ì´í„° ì €ì¥
                    saved = save_tables(tables, file_name)
                    if saved:
                        st.success(f"âœ… {len(tables)}ê°œì˜ í‘œ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

                st.success("âœ… PDF ì²˜ë¦¬ ë° ë²¡í„° DBì— ì €ì¥ ì™„ë£Œ!")
            except Exception as e:
                st.error(f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    st.sidebar.divider()

    # ë°ì´í„° ê´€ë¦¬ ì„¹ì…˜
    st.sidebar.subheader("ğŸ“Š ë°ì´í„° ê´€ë¦¬")
    if st.sidebar.button("ğŸ”„ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"):
        with st.spinner("ğŸ”„ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘..."):
            # ë²¡í„° DB ë¡œë“œ
            embeddings = get_embeddings()
            vector_store = load_vector_store(embeddings)
            if vector_store:
                st.session_state.vector_store = vector_store
                st.sidebar.success("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ!")
            
            # í‘œ ë°ì´í„° ë¡œë“œ
            tables = load_saved_tables()
            if tables:
                st.session_state.tables = tables
                st.sidebar.success(f"âœ… {len(tables)}ê°œì˜ í‘œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")

    # ë°ì´í„° ì‚­ì œ ì˜µì…˜
    if st.sidebar.button("âš ï¸ ëª¨ë“  ì €ì¥ ë°ì´í„° ì‚­ì œ"):
        try:
            # ë²¡í„° DB íŒŒì¼ ì‚­ì œ
            for file in glob.glob(os.path.join(CHROMA_DIR, "*")):
                os.remove(file)
            
            # í‘œ ë°ì´í„° íŒŒì¼ ì‚­ì œ
            for file in glob.glob(os.path.join(TABLES_DIR, "*")):
                os.remove(file)
                
            st.sidebar.success("âœ… ëª¨ë“  ì €ì¥ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.experimental_rerun()  # ì•± ì¬ì‹œì‘
        except Exception as e:
            st.sidebar.error(f"âŒ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    st.sidebar.markdown("---")
    st.sidebar.markdown("### ëª¨ë¸ ì •ë³´")
    st.sidebar.write(f"ğŸ›  í˜„ì¬ ëª¨ë¸: {selected_model}")

    return selected_model, k_value, lambda_mult_value

@st.cache_resource
def get_chat_model(model_name):
    return ChatOllama(model=model_name, streaming=True, temperature=0)

@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(model_name="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
                                 model_kwargs={'device': 'cpu'},
                                 encode_kwargs={'normalize_embeddings': True})

def load_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_file_path = tmp_file.name
    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()
    os.unlink(tmp_file_path)
    return documents

def split_text(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=250)
    texts = text_splitter.split_documents(documents)
    return texts

def handle_user_input(rag_chain, retriever, k_value):
    if prompt := st.chat_input("ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append(HumanMessage(content=prompt))
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                retrieved_docs = retriever.get_relevant_documents(prompt)

                # ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ë³µ ì œê±°
                unique_docs = []
                seen_texts = set()
                for doc in retrieved_docs:
                    if doc.page_content not in seen_texts:
                        unique_docs.append(doc)
                        seen_texts.add(doc.page_content)

                # ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë¶€ì¡±í•˜ë©´ í™•ì¥ ê²€ìƒ‰ ì‹¤í–‰
                if len(unique_docs) < k_value:
                    st.warning("ğŸ” ê´€ë ¨ ë¬¸ì„œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¶”ê°€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
                    all_docs = retriever.vectorstore.similarity_search(prompt, k=k_value * 3)

                    for doc in all_docs:
                        if doc.page_content not in seen_texts:
                            unique_docs.append(doc)
                            seen_texts.add(doc.page_content)
                        if len(unique_docs) >= k_value:
                            break  # k_value ê°œìˆ˜ë§Œí¼ë§Œ ìœ ì§€

                # ğŸ“Š í‘œ ë°ì´í„° ê²€ìƒ‰
                matching_tables = None
                if "tables" in st.session_state:
                    matching_tables = search_in_tables(st.session_state.tables, prompt)

                # ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ë³´ê¸°
                with st.expander("ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ë³´ê¸°"):
                    if unique_docs:
                        for idx, doc in enumerate(unique_docs):
                            st.markdown(f"**ğŸ“„ ë¬¸ì„œ {idx+1}:**")
                            st.write(f"ğŸ“Œ **ë©”íƒ€ë°ì´í„°:** {doc.metadata}")
                            st.write(f"ğŸ”¹ {doc.page_content[:500]}...")
                    else:
                        st.write("âŒ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

                # ğŸ“Š ê²€ìƒ‰ëœ í‘œ ë°ì´í„° ë³´ê¸°
                with st.expander("ğŸ“Š ê²€ìƒ‰ëœ í‘œ ë°ì´í„° ë³´ê¸°"):
                    if matching_tables:
                        for idx, df in matching_tables:
                            st.markdown(f"**ğŸ“Š í‘œ {idx+1}:**")
                            st.dataframe(df)
                    else:
                        st.write("ğŸ” ê´€ë ¨ëœ í‘œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                # ğŸ”¥ RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê²€ìƒ‰ëœ ë¬¸ì„œ + í‘œ ë°ì´í„° í¬í•¨)
                combined_context = "\n\n".join([
                    f"ğŸ“„ ë¬¸ì„œ {idx+1}:\n{doc.page_content}" for idx, doc in enumerate(unique_docs)
                ])
                if matching_tables:
                    for idx, df in matching_tables:
                        combined_context += f"\n\nğŸ“Š í‘œ {idx+1}:\n{df.to_string(index=False)}"

                enhanced_prompt = f"""
                You are an assistant for answering questions based on the following documents and tables.
                Make sure to include information from all relevant sources in your answer.

                Question: {prompt}
                Context: {combined_context}
                Answer:
                """

                response = rag_chain.invoke({"question": prompt, "context": enhanced_prompt})
                full_response = response
                message_placeholder.markdown(full_response)
                st.session_state.messages.append(AIMessage(content=full_response))

            except Exception as e:
                message_placeholder.markdown("âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
                st.error(f"ğŸš¨ ì˜¤ë¥˜: {e}")


def display_extracted_tables():
    """ê° í‘œë¥¼ ê°œë³„ì ìœ¼ë¡œ UIì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í‘œì‹œ"""
    if "tables" in st.session_state and st.session_state.tables:
        st.subheader("ğŸ“Š ì¶”ì¶œëœ í‘œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        for idx, df in enumerate(st.session_state.tables):
            with st.expander(f"ğŸ“Š í‘œ {idx+1} ë¯¸ë¦¬ë³´ê¸°"):
                st.dataframe(df)
    else:
        st.write("ğŸ” í˜„ì¬ ì¶”ì¶œëœ í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def main():
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    create_data_directories()
    
    configure_page()
    selected_model, k_value, lambda_mult_value = handle_sidebar()
    chat_model = get_chat_model(selected_model)

    # ğŸ“Š í‘œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€
    display_extracted_tables()

    if "messages" not in st.session_state:
        st.session_state.messages = [SystemMessage(content="ğŸ¤– AI ì±—ë´‡")]

    # ë²¡í„° ìŠ¤í† ì–´ê°€ ì„¸ì…˜ì— ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
    if "vector_store" not in st.session_state:
        embeddings = get_embeddings()
        vector_store = load_vector_store(embeddings)
        if vector_store:
            st.session_state.vector_store = vector_store
    
    # í‘œ ë°ì´í„°ê°€ ì„¸ì…˜ì— ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
    if "tables" not in st.session_state:
        tables = load_saved_tables()
        if tables:
            st.session_state.tables = tables

    # ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ RAG ì²´ì¸ ì¤€ë¹„ ë° ì§ˆë¬¸ ì²˜ë¦¬
    if "vector_store" in st.session_state:
        retriever = st.session_state.vector_store.as_retriever(
            search_type="mmr", search_kwargs={"k": k_value, "lambda_mult": lambda_mult_value}
        )

        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
            You are an assistant for question-answering tasks.
            Only use the following pieces of retrieved context to answer the question.
            Do not make up information. If the answer is not contained within the provided context, say "I don't know."
            Use five sentences maximum and keep the answer concise.
            Please answer in Korean and use respectful language.

            Question: {question}
            Context: {context}
            Answer:
            """
        )

        rag_chain = (
            {"context": RunnablePassthrough(), "question": RunnablePassthrough()}
            | prompt
            | chat_model
            | StrOutputParser()
        )

        handle_user_input(rag_chain, retriever, k_value)

    else:
        st.info("ğŸ“Œ í˜„ì¬ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        st.write("ğŸ’¡ ë˜ëŠ” ì‚¬ì´ë“œë°”ì—ì„œ 'ê¸°ì¡´ ë°ì´í„° ë¡œë“œ' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
